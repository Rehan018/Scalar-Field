{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC Filings QA Agent - Evaluation\n",
    "\n",
    "This notebook evaluates the system's performance on the 10 sample questions from the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "from main import SECFilingsQA\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the QA system\n",
    "qa_system = SECFilingsQA()\n",
    "\n",
    "# Check if system is ready\n",
    "status = qa_system.get_system_status()\n",
    "print(f\"System ready: {status['system_ready']}\")\n",
    "\n",
    "if not status['system_ready']:\n",
    "    print(\"Setting up system...\")\n",
    "    qa_system.setup_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Questions\n",
    "\n",
    "The 10 sample questions from the assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_questions = [\n",
    "    \"What are the primary revenue drivers for major technology companies, and how have they evolved?\",\n",
    "    \"Compare R&D spending trends across companies. What insights about innovation investment strategies?\",\n",
    "    \"Identify significant working capital changes for financial services companies and driving factors.\",\n",
    "    \"What are the most commonly cited risk factors across industries? How do same-sector companies prioritize differently?\",\n",
    "    \"How do companies describe climate-related risks? Notable industry differences?\",\n",
    "    \"Analyze recent executive compensation changes. What trends emerge?\",\n",
    "    \"What significant insider trading activity occurred? What might this indicate?\",\n",
    "    \"How are companies positioning regarding AI and automation? Strategic approaches?\",\n",
    "    \"Identify recent M&A activity. What strategic rationale do companies provide?\",\n",
    "    \"How do companies describe competitive advantages? What themes emerge?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "\n",
    "for i, question in enumerate(evaluation_questions, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = qa_system.query(question)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nAnswer: {result['answer']}\")\n",
    "    print(f\"\\nConfidence: {result['confidence']:.2f}\")\n",
    "    print(f\"Response Time: {response_time:.2f} seconds\")\n",
    "    print(f\"Sources: {len(result.get('sources', []))}\")\n",
    "    \n",
    "    # Store results\n",
    "    evaluation_results.append({\n",
    "        \"question_number\": i,\n",
    "        \"question\": question,\n",
    "        \"answer\": result['answer'],\n",
    "        \"confidence\": result['confidence'],\n",
    "        \"response_time\": response_time,\n",
    "        \"num_sources\": len(result.get('sources', [])),\n",
    "        \"status\": result['status'],\n",
    "        \"query_type\": result.get('query_type', 'unknown')\n",
    "    })\n",
    "    \n",
    "    # Brief pause between questions\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "successful_answers = [r for r in evaluation_results if r['status'] == 'success']\n",
    "avg_confidence = sum(r['confidence'] for r in successful_answers) / len(successful_answers) if successful_answers else 0\n",
    "avg_response_time = sum(r['response_time'] for r in evaluation_results) / len(evaluation_results)\n",
    "avg_sources = sum(r['num_sources'] for r in evaluation_results) / len(evaluation_results)\n",
    "\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Questions: {len(evaluation_questions)}\")\n",
    "print(f\"Successful Answers: {len(successful_answers)}\")\n",
    "print(f\"Success Rate: {len(successful_answers)/len(evaluation_questions)*100:.1f}%\")\n",
    "print(f\"Average Confidence: {avg_confidence:.2f}\")\n",
    "print(f\"Average Response Time: {avg_response_time:.2f} seconds\")\n",
    "print(f\"Average Sources per Answer: {avg_sources:.1f}\")\n",
    "\n",
    "# Query type breakdown\n",
    "query_types = {}\n",
    "for result in evaluation_results:\n",
    "    qt = result['query_type']\n",
    "    query_types[qt] = query_types.get(qt, 0) + 1\n",
    "\n",
    "print(\"\\nQuery Type Breakdown:\")\n",
    "for qt, count in query_types.items():\n",
    "    print(f\"- {qt}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "with open('../data/evaluation_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"evaluation_summary\": {\n",
    "            \"total_questions\": len(evaluation_questions),\n",
    "            \"successful_answers\": len(successful_answers),\n",
    "            \"success_rate\": len(successful_answers)/len(evaluation_questions)*100,\n",
    "            \"average_confidence\": avg_confidence,\n",
    "            \"average_response_time\": avg_response_time,\n",
    "            \"average_sources\": avg_sources,\n",
    "            \"query_type_breakdown\": query_types\n",
    "        },\n",
    "        \"detailed_results\": evaluation_results\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Results saved to ../data/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by question\n",
    "print(\"INDIVIDUAL QUESTION PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in evaluation_results:\n",
    "    print(f\"\\nQ{result['question_number']}: {result['question'][:60]}...\")\n",
    "    print(f\"Status: {result['status']} | Confidence: {result['confidence']:.2f} | Time: {result['response_time']:.1f}s | Sources: {result['num_sources']}\")\n",
    "    \n",
    "    if result['confidence'] < 0.5:\n",
    "        print(\"⚠️  Low confidence answer\")\n",
    "    if result['response_time'] > 30:\n",
    "        print(\"⚠️  Slow response time\")\n",
    "    if result['num_sources'] < 3:\n",
    "        print(\"⚠️  Few sources used\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}